# VLM Parser

> **AI-powered screenshot analysis tool** that combines YOLO object detection with Vision Language Models to extract UI elements with structured data.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸŒŸ What It Does

**Input:** Screenshot image  
**Output:** Structured JSON with UI elements, coordinates, and descriptions

```json
{
  "elements": [
    {
      "id": 1,
      "bbox": [100, 50, 200, 100],
      "confidence": 0.85,
      "name": "Login Button",
      "type": "Button",
      "use": "User authentication",
      "description": "Blue login button in top-right corner"
    }
  ]
}
```

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ¯ **YOLO Detection** | Fast UI element detection with precise bounding boxes |
| ğŸ¤– **Multi-VLM Support** | GPT-4o, Claude, Gemini, Ollama (local models) |
| ğŸ”“ **No Model Restrictions** | Use any model name - future-proof design |
| ğŸ“Š **Structured Output** | JSON with coordinates, types, and descriptions |
| ğŸ–¼ï¸ **Visual Annotations** | Saves annotated images with numbered elements |
| ğŸš€ **Production Ready** | CLI tool, Python API, batch processing |
| ğŸ  **Local Models** | Private analysis with Ollama (free & offline) |

## ğŸš€ Quick Start

### 1. Installation
```bash
git clone <repository>
cd myomni
pip install -r requirements.txt
```

### 2. Configure API Keys
```bash
cp .env.example .env
# Edit .env with your API keys
```

### 3. Run Analysis
```bash
# Interactive mode (recommended for first time)
python main.py

# Direct CLI
python cli.py screenshot.png --vlm gpt4o

# With custom model
python cli.py screenshot.png --vlm openai --model gpt-4-turbo-preview
```

## ğŸ“– Usage Guide

### Command Line Interface
```bash
# Basic usage
python cli.py image.png --vlm gpt4o

# Batch processing
python cli.py *.png --batch --output results/

# Custom model
python cli.py image.png --vlm claude --model claude-3-opus-latest

# Local model (free)
python cli.py image.png --vlm ollama --model llava:latest

# JSON output only
python cli.py image.png --json-only
```

### Python API
```python
from src.vlm_parser import VLMParser

# Initialize parser
parser = VLMParser(vlm_type='gpt4o')

# Analyze single image
results = parser.analyze('screenshot.png')
print(f"Found {len(results['elements'])} UI elements")

# Batch processing
results = parser.analyze_batch(['img1.png', 'img2.png'])

# Custom model
parser = VLMParser(vlm_type='openai', model='gpt-4-turbo-preview')
results = parser.analyze('screenshot.png')
```

### Interactive Mode
```bash
python main.py
# Follow the prompts to:
# 1. Choose VLM provider
# 2. Enter image path
# 3. Get instant analysis
```

## ğŸ  Local Models (Free & Private)

Run vision models locally with **zero API costs**:

```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Pull a vision model
ollama pull llava:latest         # 4.5GB - Best overall
ollama pull minicpm-v:latest     # 2.8GB - Fast & efficient  
ollama pull moondream:latest     # 1.7GB - Lightweight

# 3. Start Ollama
ollama serve

# 4. Use with VLM Parser
python main.py  # Choose option 4 (Ollama)
```

**Benefits:** Free, private, offline, no API limits

## ğŸ”§ Configuration

### Environment Variables (.env)
```bash
# API Keys
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
GOOGLE_API_KEY=your-google-api-key

# Default settings
VLM_PROVIDER=gpt4o
```

### Config File (config.json)
```json
{
  "vlm_type": "gpt4o",
  "default_models": {
    "openai": "gpt-4o",
    "anthropic": "claude-3-5-sonnet-20241022",
    "google": "gemini-1.5-flash",
    "ollama": "llava:latest"
  },
  "yolo_config": {
    "box_threshold": 0.05,
    "iou_threshold": 0.1
  }
}
```

## ğŸ¯ Model Flexibility

**âœ… No Restrictions** - Use any model name without code changes:

```bash
# Latest GPT models (even future ones)
--model gpt-4-turbo-preview
--model gpt-5-ultra

# Custom Claude models
--model claude-3-opus-latest
--model claude-4-sonnet

# Google's newest models  
--model gemini-2.0-flash-exp
--model gemini-3.0-ultra

# Local Ollama models
--model llava:13b
--model custom-vision-model:latest
```

## ğŸ“ Project Structure

```
myomni/
â”œâ”€â”€ src/                    # Core modules
â”‚   â”œâ”€â”€ vlm_parser.py      # Main API class
â”‚   â”œâ”€â”€ yolo_detector.py   # YOLO detection engine
â”‚   â”œâ”€â”€ vlm_clients.py     # Multi-VLM integrations
â”‚   â””â”€â”€ __init__.py        # Package exports
â”œâ”€â”€ cli.py                 # Command line interface
â”œâ”€â”€ main.py               # Interactive mode
â”œâ”€â”€ config.json           # Configuration settings
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env                  # API keys (create from .env.example)
â”œâ”€â”€ .env.example         # Template for API keys
â””â”€â”€ weights/             # YOLO model weights
    â””â”€â”€ icon_detect/
        â””â”€â”€ model.pt     # Pre-trained YOLO model
```

## ğŸ“‹ Requirements

- **Python:** 3.8 or higher
- **YOLO Model:** Place model weights in `weights/icon_detect/model.pt`
- **API Keys:** At least one VLM provider API key (or use Ollama locally)
- **Memory:** 4GB RAM minimum (8GB recommended for local models)

## ğŸ” Output Examples

### Console Output
```
ğŸ” Analyzing screenshot.png...
âœ… YOLO detected 3 UI elements
ğŸ¤– Analyzing with GPT-4o...

Element 1: Login Button
  ğŸ“ Location: (892, 24) to (956, 44)
  ğŸ¯ Confidence: 0.91
  ğŸ“ Blue authentication button in header

Element 2: Search Field  
  ğŸ“ Location: (300, 100) to (600, 130)
  ğŸ¯ Confidence: 0.87
  ğŸ“ Text input for search queries

ğŸ’¾ Results saved to: screenshot_results.json
ğŸ–¼ï¸ Annotated image: screenshot_analyzed.png
```

### JSON Output
```json
{
  "success": true,
  "elements": [
    {
      "id": 1,
      "bbox": [892, 24, 956, 44],
      "confidence": 0.91,
      "name": "Login Button",
      "type": "Button", 
      "use": "User authentication",
      "description": "Blue authentication button in header"
    }
  ],
  "yolo_detections": 3,
  "vlm_type": "gpt4o",
  "model": "gpt-4o"
}
```

## ğŸ› ï¸ Advanced Usage

### Batch Processing
```python
from src.vlm_parser import VLMParser

parser = VLMParser(vlm_type='gpt4o')
results = parser.analyze_batch([
    'screen1.png', 'screen2.png', 'screen3.png'
], output_dir='results/')

print(f"Processed {results['summary']['successful']} images")
```

### Custom YOLO Settings
```python
parser = VLMParser(
    vlm_type='gpt4o',
    confidence_threshold=0.3,  # Higher threshold
    iou_threshold=0.5,         # Different IoU
    yolo_model_path='custom_model.pt'
)
```

### Export Results
```python
# Export to CSV
parser.export_results(results, 'output.csv', format='csv')

# Export to JSON  
parser.export_results(results, 'output.json', format='json')
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹â€â™‚ï¸ Support

- **Issues:** [GitHub Issues](https://github.com/yourusername/myomni/issues)
- **Discussions:** [GitHub Discussions](https://github.com/yourusername/myomni/discussions)
- **Email:** support@vlmparser.com

---

**Made with â¤ï¸ for the AI community**