# VisionParse

> **AI-powered screenshot analysis tool** that combines YOLO object detection with Vision Language Models to extract UI elements with structured data. Now powered by **LlamaIndex** for unified multimodal AI integration.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![LlamaIndex](https://img.shields.io/badge/Powered%20by-LlamaIndex-orange.svg)](https://www.llamaindex.ai/)

## üåü What It Does

**Input:** Screenshot image  
**Output:** Structured JSON with UI elements, coordinates, and descriptions

```json
{
  "elements": [
    {
      "id": 1,
      "bbox": [100, 50, 200, 100],
      "confidence": 0.85,
      "name": "Login Button",
      "type": "Button",
      "use": "User authentication",
      "description": "Blue login button in top-right corner"
    }
  ]
}
```

## ‚ú® Key Features

| Feature | Description |
|---------|-------------|
| üéØ **YOLO Detection** | Fast UI element detection with precise bounding boxes |
| ü§ñ **LlamaIndex Integration** | Unified interface for GPT-4o, Claude, Gemini, Ollama |
| üîì **No Model Restrictions** | Use any model name - future-proof design |
| üìä **Structured Output** | JSON with coordinates, types, and descriptions |
| üñºÔ∏è **Visual Annotations** | Saves annotated images with numbered elements |
| üöÄ **Production Ready** | Python library, batch processing |
| üè† **Local Models** | Private analysis with Ollama (free & offline) |

## üî• What's New: LlamaIndex Integration

VisionParse now uses **LlamaIndex** for all VLM interactions, providing:

- **Unified API**: Single interface for all providers instead of multiple libraries
- **Better Error Handling**: Built-in retry logic and timeout management
- **Cleaner Code**: Simplified provider implementations
- **Future-Proof**: Easy to add new models as LlamaIndex supports them

## üöÄ Quick Start

### 1. Installation
```bash
git clone https://github.com/MaharshPatelX/VisionParse.git
cd VisionParse
pip install -r requirements.txt
```

### 2. Configure API Keys
Create a `.env` file in the project root:
```bash
# API Keys
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
GOOGLE_API_KEY=your-google-api-key
```

### 3. Use as Library
```python
from VisionParse import VisionParse

# Initialize with provider + model
parser = VisionParse(provider='openai', model='gpt-4o')
results = parser.analyze('screenshot.png')

print(f"Found {len(results['elements'])} UI elements")
```

### 4. Test the Setup
```bash
python test.py
```

## üìñ Usage Guide

### Python Library
```python
from VisionParse import VisionParse

# Initialize parser with provider + model
parser = VisionParse(provider='openai', model='gpt-4o')

# Analyze single image
results = parser.analyze('screenshot.png')
print(f"Found {len(results['elements'])} UI elements")

# Different providers
parser = VisionParse(provider='anthropic', model='claude-3-5-sonnet')
parser = VisionParse(provider='google', model='gemini-2.0-flash-exp')
parser = VisionParse(provider='ollama', model='llava:latest')

# Batch processing
results = parser.analyze_batch(['img1.png', 'img2.png'])

# Custom thresholds
parser = VisionParse(
    provider='openai', 
    model='gpt-4o',
    confidence_threshold=0.3,
    iou_threshold=0.5
)
```

## üè† Local Models (Free & Private)

Run vision models locally with **zero API costs**:

```bash
# 1. Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. Pull a vision model
ollama pull llava:latest         # 4.5GB - Best overall
ollama pull minicpm-v:latest     # 2.8GB - Fast & efficient  
ollama pull moondream:latest     # 1.7GB - Lightweight

# 3. Start Ollama
ollama serve

# 4. Use with VisionParse
parser = VisionParse(provider='ollama', model='llava:latest')
results = parser.analyze('screenshot.png')
```

**Benefits:** Free, private, offline, no API limits

## üîß Configuration

### Environment Variables (.env)
```bash
# API Keys
OPENAI_API_KEY=sk-your-openai-key
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key
GOOGLE_API_KEY=your-google-api-key

# Default settings (optional)
VLM_PROVIDER=openai
```

### Config File (config.json)
```json
{
  "provider": "openai",
  "default_models": {
    "openai": "gpt-4o",
    "anthropic": "claude-3-5-sonnet",
    "google": "gemini-2.0-flash-exp",
    "ollama": "llava:latest"
  },
  "yolo_config": {
    "box_threshold": 0.05,
    "iou_threshold": 0.1
  }
}
```

## üéØ Model Flexibility

**‚úÖ No Restrictions** - Use any model name without code changes:

```bash
# Latest OpenAI models (2025)
--model gpt-4.1
--model gpt-4.1-mini
--model gpt-4o

# Latest Anthropic Claude models (2025)
--model claude-4-opus
--model claude-4-sonnet
--model claude-3.5-sonnet

# Latest Google Gemini models (2025)
--model gemini-2.5-pro
--model gemini-2.0-flash

# Local Ollama models
--model llava:latest
--model minicpm-v:latest
--model moondream:latest
```

## üìÅ Project Structure

```
VisionParse/
‚îú‚îÄ‚îÄ VisionParse/            # Main package
‚îÇ   ‚îú‚îÄ‚îÄ src/               # Core modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vlm_parser.py  # Main API class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolo_detector.py # YOLO detection engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vlm_clients.py # LlamaIndex VLM integrations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py    # Module exports
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py        # Package exports
‚îú‚îÄ‚îÄ test.py               # Test script for LlamaIndex integration
‚îú‚îÄ‚îÄ config.json           # Configuration settings
‚îú‚îÄ‚îÄ requirements.txt      # Python dependencies (now with LlamaIndex)
‚îú‚îÄ‚îÄ .env                  # API keys (create from .env.example)
‚îú‚îÄ‚îÄ weights/             # YOLO model weights
‚îÇ   ‚îî‚îÄ‚îÄ icon_detect/
‚îÇ       ‚îî‚îÄ‚îÄ model.pt     # Pre-trained YOLO model
‚îî‚îÄ‚îÄ README.md            # This file
```

## üìã Requirements

- **Python:** 3.8 or higher
- **LlamaIndex:** Core and provider packages
- **YOLO Model:** Place model weights in `weights/icon_detect/model.pt`
- **API Keys:** At least one VLM provider API key (or use Ollama locally)
- **Memory:** 4GB RAM minimum (8GB recommended for local models)

## üîç Dependencies

### Core ML Dependencies
```
torch>=2.0.0
torchvision>=0.15.0
ultralytics>=8.0.0
Pillow>=9.0.0
opencv-python>=4.8.0
numpy>=1.24.0
```

### LlamaIndex LLM Clients (Unified Interface)
```
llama-index-core>=0.11.0
llama-index-llms-openai>=0.2.0
llama-index-llms-anthropic>=0.3.0
llama-index-llms-gemini>=0.2.0
llama-index-llms-ollama>=0.3.0
```

## üîç Output Examples

### Python Output
```python
results = parser.analyze('screenshot.png')
print(f"Found {len(results['elements'])} UI elements")

for element in results['elements']:
    print(f"- {element['name']}: {element['description']}")

# Output:
# Found 3 UI elements
# - Login Button: Blue authentication button in header
# - Search Field: Text input for search queries
# - Menu Icon: Navigation menu toggle
```

### JSON Output
```json
{
  "success": true,
  "elements": [
    {
      "id": 1,
      "bbox": [892, 24, 956, 44],
      "confidence": 0.91,
      "name": "Login Button",
      "type": "Button", 
      "use": "User authentication",
      "description": "Blue authentication button in header"
    }
  ],
  "yolo_detections": 3,
  "vlm_type": "openai",
  "model": "gpt-4o"
}
```

## üìö Library Usage

### Installation as Library
```bash
# Install from local directory
cd VisionParse
pip install -e .

# Or install normally
pip install .

# Install directly from GitHub
pip install git+https://github.com/MaharshPatelX/VisionParse.git
```

### Integration in Your Application
```python
import os
from VisionParse import VisionParse, VisionParseError

class ScreenshotAnalyzer:
    def __init__(self, provider='openai', model='gpt-4o'):
        self.parser = VisionParse(
            provider=provider,
            model=model,
            verbose=False
        )
    
    def analyze_ui(self, image_path):
        """Analyze UI elements in screenshot"""
        try:
            results = self.parser.analyze(image_path)
            
            if results['success']:
                return {
                    'elements': results['elements'],
                    'count': len(results['elements']),
                    'confidence_avg': sum(el['confidence'] for el in results['elements']) / len(results['elements'])
                }
            else:
                return {'error': results['error']}
                
        except VisionParseError as e:
            return {'error': str(e)}

# Usage
analyzer = ScreenshotAnalyzer(provider='anthropic', model='claude-3-5-sonnet')
ui_elements = analyzer.analyze_ui('app_screenshot.png')
```

## üõ†Ô∏è Advanced Usage

### Batch Processing
```python
parser = VisionParse(provider='openai', model='gpt-4o')
results = parser.analyze_batch([
    'screen1.png', 'screen2.png', 'screen3.png'
], output_dir='results/')

print(f"Processed {results['summary']['successful']} images")
```

### Custom YOLO Settings
```python
# Dynamic threshold adjustment
parser = VisionParse(provider='openai', model='gpt-4o')

# More sensitive detection
parser.confidence_threshold = 0.1
parser.iou_threshold = 0.3
results = parser.analyze('screenshot.png')

# Stricter detection
parser.confidence_threshold = 0.7
parser.iou_threshold = 0.8
results = parser.analyze('screenshot.png')
```

### Export Results
```python
# Export to CSV
parser.export_results(results, 'output.csv', format='csv')

# Export to JSON  
parser.export_results(results, 'output.json', format='json')
```

## üß™ Testing

Run the comprehensive test suite:
```bash
python test.py
```

The test will:
- ‚úÖ Check LlamaIndex package installations
- ‚úÖ Verify API key configuration from .env
- ‚úÖ Test all provider integrations
- ‚úÖ Validate VisionParse initialization

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly with `python test.py`
5. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôã‚Äç‚ôÇÔ∏è Support

- **Issues:** [GitHub Issues](https://github.com/MaharshPatelX/VisionParse/issues)
- **Discussions:** [GitHub Discussions](https://github.com/MaharshPatelX/VisionParse/discussions)

## üèÜ Powered By

- **[LlamaIndex](https://www.llamaindex.ai/)** - Unified multimodal AI framework
- **[Ultralytics YOLO](https://ultralytics.com/)** - Object detection
- **[OpenAI](https://openai.com/)** - GPT-4o vision models
- **[Anthropic](https://anthropic.com/)** - Claude vision models  
- **[Google](https://ai.google.dev/)** - Gemini vision models
- **[Ollama](https://ollama.com/)** - Local model inference

---

**Made with ‚ù§Ô∏è for the AI community** | **Now with LlamaIndex integration for better multimodal AI**